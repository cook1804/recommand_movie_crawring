{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TF-IDF.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGIXLmGj_oFC"
      },
      "source": [
        "### ** TF-IDF 실습 **"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-6q3oQa_hSR"
      },
      "source": [
        "%%time\n",
        "path=\"/content/drive/MyDrive/Colab Notebooks/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ne-CVsTSDTSh"
      },
      "source": [
        "import kss\n",
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFYkjN0mBIFI"
      },
      "source": [
        "removal_list =  \"‘, ’, ◇, ‘, ”,  ’, ', ·, \\“, ·, △, ●,  , ■, (, ), \\\", >>, `, /, -,∼,=,ㆍ<,>, .,?, !,【,】, …, ◆,%\"\n",
        "def cleansing_special(sentence):\n",
        "    # 특수문자를 전처리를 하는 함수\n",
        "    sentence = re.sub(\"[.,\\'\\\"’‘”“!?]\", \"\", sentence)\n",
        "    sentence = re.sub(\"[^가-힣0-9a-zA-Z\\\\s]\", \" \", sentence)\n",
        "    sentence = re.sub(\"\\s+\", \" \", sentence)\n",
        "    sentence = sentence.translate(str.maketrans(removal_list, ' '*len(removal_list)))\n",
        "    sentence = sentence.strip()\n",
        "    return sentence\n",
        "def cleansing_numbers(sentence):\n",
        "    # 숫자를 전처리(delexicalization) 하는 함수    \n",
        "    sentence = re.sub('[0-9]+', 'NUM', sentence)\n",
        "    sentence = re.sub('NUM\\s+', \"NUM\", sentence)\n",
        "    sentence = re.sub('[NUM]+', \"NUM\", sentence)\n",
        "    return sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfP-zFNXBsAK"
      },
      "source": [
        "import os\n",
        "from konlpy.tag import Mecab\n",
        "mecab = Mecab()\n",
        "all_sentences = []\n",
        "with open(os.path.join(path, 'news_sample.txt'),'r', encoding='utf-8') as f:\n",
        "    for idx, line in enumerate(f.readlines()):\n",
        "        print(f\"---문서 {idx}번 ---\")\n",
        "        preprocessed = cleansing_numbers(line)\n",
        "        preprocessed = cleansing_numbers(preprocessed)\n",
        "        # 명사만 추출\n",
        "        preprocessed_news = ' '.join(list(set(mecab.nouns(preprocessed))))\n",
        "        # 문장으로 분리\n",
        "        # preprocessed_news = ' '.join(kss.split_sentences(preprocessed))\n",
        "        print(preprocessed_news)\n",
        "        all_sentences.append(preprocessed_news)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2trcZj-EuLF"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from collections import defaultdict\n",
        "\n",
        "word2id = defaultdict(lambda : 0)\n",
        "print(word2id)\n",
        "\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(all_sentences)\n",
        "for idx, feature in enumerate(tfidf_vectorizer.get_feature_names()):\n",
        "    word2id[feature] = idx\n",
        "print(word2id)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vt1adVUrHWqN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}